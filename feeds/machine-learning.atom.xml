<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>haskai - Machine learning</title><link href="https://hoff97.github.io/" rel="alternate"></link><link href="https://hoff97.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://hoff97.github.io/</id><updated>2020-08-31T16:25:00+02:00</updated><subtitle>Software Developer</subtitle><entry><title>Fast uncertainty estimation for mobilenet.</title><link href="https://hoff97.github.io/mobilenet-uncertainty.html" rel="alternate"></link><published>2020-08-31T16:25:00+02:00</published><updated>2020-08-31T16:25:00+02:00</updated><author><name>Frithjof Winkelmann</name></author><id>tag:hoff97.github.io,2020-08-31:/mobilenet-uncertainty.html</id><summary type="html">&lt;p&gt;Here I explore how uncertainty estimation can be done very quickly with mobilenet&lt;/p&gt;</summary><content type="html">&lt;p&gt;While writing my bachelor thesis, I often ended up using &lt;a href="http://detexify.kirelabs.org/classify.html"&gt;Detexify&lt;/a&gt;
to look up latex symbols I forgot. Unfortunately, mobile data doesnt work very
well in germany, so when I was traveling a lot of times I could'nt access this page.
For this reason, I build &lt;a href="https://detext.haskai.de/client/"&gt;Detext&lt;/a&gt;,
a progressive web app (PWA), that classifies latex symbols without needing
a internet connection. It uses MobileNet &lt;a href='#Howard2017' id='ref-Howard2017-1'&gt;(Howard et al., 2017)&lt;/a&gt;,
which is run directly on the client side using &lt;a href="https://github.com/microsoft/onnxjs"&gt;onnx.js&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/detext_example.png" title="Example of detext predicting the latex code for the alpha character"&gt;&lt;/p&gt;
&lt;p&gt;This works pretty well for most symbols. The only pet peeve I had with
it until recently was, that it wont tell you when its not
certain about the predicted symbol or simply predicts the wrong symbol.
In the following example, the app predicts the &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; (\rightarrow) symbol,
since it does'nt know &lt;span class="math"&gt;\(\rightharpoonup\)&lt;/span&gt; (\rightharpoonup).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/detext_wrong.png" title="Detext predicts \rightarrow since it doesnt know \rightharpoonup"&gt;&lt;/p&gt;
&lt;p&gt;Ideally of course, the prediction would include some uncertainty of the network.
This is known as uncertainty estimation and a variety of approaches exist to
solve it, for example using model ensembles &lt;a href='#Lakshminarayanan2017' id='ref-Lakshminarayanan2017-1'&gt;(Lakshminarayanan et al., 2017)&lt;/a&gt;, predicting a uncertainty &lt;a href='#Sequ2019' id='ref-Sequ2019-1'&gt;(Segu et al., 2019)&lt;/a&gt; or
using test time dropout &lt;a href='#Gal2016' id='ref-Gal2016-1'&gt;(Gal and Ghahramani, 2016)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article I will focus on test time dropout. Normally, dropout is
only enabled during training and is used to prevent overfitting.
To enable the model uncertainty, instead of disabling the dropout layers(s)
during testing, one can instead keep it enabled. Feeding in the same input
to a network with test time dropout will then give different outputs,
depending on which neurons stay enabled.&lt;/p&gt;
&lt;p&gt;Lets look at an example. The top-5 softmax values that mobilenet predicts
for the following picture
&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/rightharpoon.png" title="Hand drawn right harpoon symbol"&gt;&lt;/p&gt;
&lt;p&gt;are shown in the plot below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/wrong_softmax.png" title="Softmax of the top-5 classes"&gt;&lt;/p&gt;
&lt;p&gt;Since the &lt;span class="math"&gt;\(\rightharpoonup\)&lt;/span&gt; symbol is not in the training dataset, these
are all wrong of course. Unfortunately, the softmax score for the top class
is pretty high, so it can not be used as an uncertainty estimate.&lt;/p&gt;
&lt;p&gt;To estimate the model uncertainty, we turn on dropout at test time
and look at the model predictions. For this we pass the same image
through the network 100 times and visualize the predictions together with
their variance:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/rightharpoonerr.png" title="Softmax of the top-5 classes with error bars"&gt;&lt;/p&gt;
&lt;p&gt;Here the black bars indicate the standard deviation of the predicted
softmax values. We can see that using test time dropout induces some variance.&lt;/p&gt;
&lt;p&gt;If we compare this to the prediction of a symbol that is included in the
dataset, like the following&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hand drawn right arrow symbol" src="https://hoff97.github.io/images/mobilenet_uncertainty/rightarrow.png" title="Hand drawn right arrow symbol"&gt;&lt;/p&gt;
&lt;p&gt;we can see that this has a lower variance:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax of the top-5 classes with error bars" src="https://hoff97.github.io/images/mobilenet_uncertainty/rightarrowerr.png" title="Softmax of the top-5 classes with error bars"&gt;.&lt;/p&gt;
&lt;p&gt;Lets look at a few more examples. Here are the variances of symbols that are close to the training set:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/uncertain_good.png" title="Variances of images similar to the training set"&gt;.&lt;/p&gt;
&lt;p&gt;And here those of pictures that are not found in the training set:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/uncertain_bad.png" title="Variances of images not similar to the training set"&gt;.&lt;/p&gt;
&lt;p&gt;This is nice, since it allows us to detect when the model is uncertain about its
predictions. Unfortunately, this approach requires us to make multiple
forward passes through the model.
In the case of the Detext app this is not acceptable. The app should run on
mobile devices too, and there a forward pass might already take ~1 second.
Doing 10 forward passes just to estimate the uncertainty would take too long.
Fortunately the architecture of mobilenet allows us to estimate the variance
with only one forward pass.&lt;/p&gt;
&lt;p&gt;The standard implementation of MobileNet only includes a single dropout layer.
It computes a image feature with the feature network
&lt;span class="math"&gt;\(f: \mathbb{R}^{W \times H} \rightarrow \mathbb{R}^D\)&lt;/span&gt;, then applies dropout
and then used a single fully connected layer and a softmax, so in full the prediction
of an image is
&lt;/p&gt;
&lt;div class="math"&gt;$$ pred = softmax(W \times dropout(f(image)) $$&lt;/div&gt;
&lt;p&gt;The feature of an image will thus always be the same, even with test time dropout
enabled. We can use a few simple tricks to estimate the variance.&lt;/p&gt;
&lt;p&gt;The covariance matrix &lt;span class="math"&gt;\(\Sigma^F\)&lt;/span&gt; of &lt;span class="math"&gt;\(F = dropout(f(image))\)&lt;/span&gt; is simply a diagonal matrix
with &lt;span class="math"&gt;\(\Sigma^F_{ii} = p/(1-p)*f(image)_i^2\)&lt;/span&gt; (its just a bernoully random vector
multiplied by &lt;span class="math"&gt;\(f(image)/(1-p)\)&lt;/span&gt; ).&lt;/p&gt;
&lt;p&gt;We get the covariance after the linear layer by using the fact that
&lt;span class="math"&gt;\(X = W F\)&lt;/span&gt; has covariance &lt;span class="math"&gt;\(W \Sigma^F W^T\)&lt;/span&gt;.
Now we only have to find out how to compute
the variance of &lt;span class="math"&gt;\(softmax(X)\)&lt;/span&gt; given the covariance and mean of &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can do this using the
&lt;a href="https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables"&gt;taylor expansion for the moments of random variables&lt;/a&gt;. For the covariance
this reads:&lt;/p&gt;
&lt;div class="math"&gt;$$ Cov(f(X)) \approx J_f \Sigma^X J_f^T$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(J_f\)&lt;/span&gt; is the jacobian of &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluated at &lt;span class="math"&gt;\(E[X]\)&lt;/span&gt;.
We only need to find the jacobian of the softmax which is given by:&lt;/p&gt;
&lt;div class="math"&gt;$$ J^{softmax}_{ij}(X) = \begin{cases}
    softmax(X)_i (1 - softmax(X)_i) &amp;amp; i=j\\
    -softmax(X)_j softmax(X)_i &amp;amp; i \neq j
\end{cases} $$&lt;/div&gt;
&lt;p&gt;In total the evaluation of the variance is done by&lt;/p&gt;
&lt;div class="math"&gt;$$
F = f(image)\\
\Sigma^F_{ii} = \tfrac{p}{1-p}*F_i^2\\
X = W F\\
\Sigma^X = W \Sigma^F W^T\\
S = softmax(X)\\
\Sigma^S = J^{softmax}(X) \Sigma^X J^{softmax}(X)^T
$$&lt;/div&gt;
&lt;p&gt;Lets compare the variance we get with this estimate to the variance estimated from sampling:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/variance_comparison.png" title="Comparison of variance estimated from sampling and approximation"&gt;&lt;/p&gt;
&lt;p&gt;We can see that the approximated variance is not exactly equal to the variance estimated from sampling, but it is generally
higher for images that are not similar from the training set.&lt;/p&gt;
&lt;p&gt;This can be used as a rough indicator, when the prediction of MobileNet might not be trusted.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://detext.haskai.de/client/"&gt;Detext&lt;/a&gt; app this looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/images/mobilenet_uncertainty/detext_uncertain.png" title="Example of uncertainty estimate in detext"&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='Gal2016'&gt;Yarin Gal and Zoubin Ghahramani.
Dropout as a bayesian approximation: representing model uncertainty in deep learning.
In &lt;em&gt;Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48&lt;/em&gt;, ICML'16, 1050&amp;ndash;1059. JMLR.org, 2016.
URL: &lt;a href="https://dl.acm.org/doi/10.5555/3045390.3045502"&gt;https://dl.acm.org/doi/10.5555/3045390.3045502&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Gal2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Howard2017'&gt;Andrew&amp;nbsp;G. Howard, Menglong Zhu, Bo&amp;nbsp;Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: efficient convolutional neural networks for mobile vision applications.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1704.04861"&gt;http://arxiv.org/abs/1704.04861&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1704.04861"&gt;arXiv:1704.04861&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Howard2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Lakshminarayanan2017'&gt;Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
Simple and scalable predictive uncertainty estimation using deep ensembles.
In &lt;em&gt;Proceedings of the 31st International Conference on Neural Information Processing Systems&lt;/em&gt;, NIPS'17, 6405&amp;ndash;6416. Red Hook, NY, USA, 2017. Curran Associates Inc.
URL: &lt;a href="https://dl.acm.org/doi/10.5555/3295222.3295387"&gt;https://dl.acm.org/doi/10.5555/3295222.3295387&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Lakshminarayanan2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Sequ2019'&gt;Mattia Seg&lt;span class="bibtex-protected"&gt;&lt;span class="bibtex-protected"&gt;ù&lt;/span&gt;&lt;/span&gt;, Antonio Loquercio, and Davide Scaramuzza.
A general framework for uncertainty estimation in deep learning.
&lt;em&gt;CoRR&lt;/em&gt;, 2019.
URL: &lt;a href="http://arxiv.org/abs/1907.06890"&gt;http://arxiv.org/abs/1907.06890&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1907.06890"&gt;arXiv:1907.06890&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Sequ2019-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="Machine learning"></category><category term="deep-learning"></category><category term="uncertainty-estiamtion"></category></entry></feed>